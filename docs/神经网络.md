## 正向传播
假设我们的神经网络是由三层神经元构成：分别是一层拥有4个神经元的隐藏层，一层拥有三个神经元的隐藏层，以及最后一层只拥有一个神经元的输出层，神经网络结构如下图所示：

![](https://cdn.nlark.com/yuque/0/2025/png/45785720/1754758666679-554e9a17-89aa-44b0-abb8-52af05e59b09.png)

假设我们输入的数据的维度是2，那么第一层神经元里面的每一个神经元都有$ w_1 $和$ w_2 $两个权重以及一个偏置，整个第一层就一共有 $ 2 \times 4 = 8 $个$ w $以及四个偏置$ b $。我们用$ w^{(l)}_{ij} $表示每一层的权重，其中

+ $ (l) $表示这个权重属于第几层。
+ $ i $表示这是这一层的第几个神经元。
+ 最后$ j $表示这是这个神经元的第几个权重，也对应着前一层第几个神经元的输出。

例如，第一层的第一个神经元的权重为$ w_{11}^{(1)} $以及$ w_{12}^{(1)} $，这里的上标（1）代表了这两个权重是第一层神经元的权重；而下标的两个数字，前面一个数字“1”代表了这个第一层的第一个神经元，第二个数字“1”和“2”分别代表了这是这一个神经元的第一个权重和第二个权重。

> 在机器学习中我们还通常习惯性的把输入的数据叫做第零层，第零层通常不算做神经网络的层数。
>

第二层神经元里面每一个神经元都有四个$ w $——每个$ w $分别对应着上一层神经元的四个输出，以及一个偏置$ b^{(2)} $。而最后一层输出层的参数数量则根据需要预测的目标来决定：如果我们需要这个神经网络预测房价，一个线性预测任务，则输出层有一个神经元，神经元内有有三个权重，分别对应着上一层的三个输出；如果是决定某样商品是否畅销，一个二分类任务，则输出层应该有一个神经元，神经元内有三个权重以及Sigmoid激活函数；如果我们要识别MNIST中的手写数字，则输出层应该有10个神经元，并且使用Softmax函数作为最后一层的激活函数。

| 任务 | 输出层神经元数量 | 输出层激活函数 | 损失函数 |
| --- | --- | --- | --- |
| 房价 | 1 | 无/线性，y=x | 均方误差（MSE） |
| 二分类问题 | 1 | Sigmoid | 交叉熵损失函数（CEL） |
| 多分类问题（例如MNIST） | K（MNIST中K=10） | Softmax | <font style="color:rgb(64, 64, 64);">分类交叉熵</font> |


那么神经网络是怎么做出预测的呢？神经网络会逐层的计算每一层的结果，最后到输出层，我们把这个过程叫做正向传播（或者前向传播，forward propagation）。在上例中，我们的神经网络的输入为一个二维的数据，假设为$ x_1 $和$ x_2 $，那么我们首先计算第一层神经元的输出：

第一层有四个神经元，我们分别计算每个神经元与我们的输入相乘：

$ z^{(1)}_1 = w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + b^{(1)}_1 $

$ z^{(1)}_2 = w^{(1)}_{21}x_1 + w^{(1)}_{22}x_2 + b^{(1)}_2 $

$ z^{(1)}_3 = w^{(1)}_{31}x_1 + w^{(1)}_{32}x_2 + b^{(1)}_3 $

$ z^{(1)}_4 = w^{(1)}_{41}x_1 + w^{(1)}_{42}x_2 + b^{(1)}_4 $

然后对每一个输出应用激活函数 $ h(x) $：

$ a^{(1)}_1 = h(z^{(1)}_1) \\
... \\
a^{(1)}_4 = h(z^{(1)}_4)
 $

现在我们有了$ a^{(1)}_1 .. a^{(1)}_4 $四个输出，在全连接神经网络中，这一层的所有神经元的输出，都将作为下一层神经元的输入。与之相对的是稀疏连接神经网络，在稀疏连接神经网络中，下一层的每一个神经元只连接上一层中离其较近的几个神经元。稀疏神经网络较全连接神经网络相比，需要计算的数据更少，在特定数据集上的表现要优于全连接神经网络。在这里，我们以最常见的全连接神经网络为例，计算下一层的输出——在第二层中，我们一共有三个神经元，每一个神经元都接收来自上一层四个神经元的输出，我们计算每一个神经元的权重与上一层输出的乘积加偏置：

$ z^{(2)}_1 = w^{(2)}_{11}a^{(1)}_1 + w^{(2)}_{12}a^{(1)}_2 + w^{(2)}_{13}a^{(1)}_3 + w^{(2)}_{14}a^{(1)}_4 + b^{(2)}_1 $

$ z^{(2)}_2 = w^{(2)}_{21}a^{(1)}_1 + w^{(2)}_{22}a^{(1)}_2 + w^{(2)}_{23}a^{(1)}_3 + w^{(2)}_{24}a^{(1)}_4 + b^{(2)}_2 $

$ z^{(2)}_3 = w^{(2)}_{31}a^{(1)}_1 + w^{(2)}_{32}a^{(1)}_2 + w^{(2)}_{33}a^{(1)}_3 + w^{(2)}_{34}a^{(1)}_4 + b^{(2)}_3 $

然后对每一个输出应用激活函数$ h(x) $

$ a^{(2)}_1 = h(z^{(2)}_1) \\
... \\
a^{(2)}_3 = h(z^{(2)}_3)  $

第二层神经元输出了 $ a^{(2)}_1 , a^{(2)}_2, a^{(2)}_3 $三个输出，这些输出会用送到最后一层输出层神经元，用于做最后的决策。在这里，我们假设我们面对的问题是一个二分类问题，那么最后一个层输出层应该只有一个神经元，并且使用激活函数Softmax——最后我们计算：

$ z^{(3)}_1 = w^{(3)}_{11} a^{(2)}_1 + w^{(3)}_{12} a^{(2)}_2 + w^{(3)}_{13} a^{(2)}_3 + b^{(3)}_1 $

最后使用Softmax函数，输出一个预测标签为1的概率：

$ \hat{y} = \frac{1}{1 + e^{-z^{(3)}_1}} $

如果我们面对的是一个多分类问题，则最后一层应该有 K 个神经元（K为实际标签类别的数量），然后再在K个神经元的输出上使用 Softmax 激活函数。

## 反向传播
接着我们更新权重参数，我们分别计算损失函数在每一个权重和偏置方向上的偏导。如果我们要预测的目标是一个线性问题，那么我们的损失函数为均方差MSE函数；如果我们需要预测的目标是一个二分类问题，那么我们的损失函数则应该是交叉熵损失函数CEL；如果是多分类问题，则应该是是稀疏交叉熵函数SCEL。

在这里，我们先以二分类问题举例，假设我们的损失函数为交叉熵损失函数：

$ L(\hat{y},y) = -y\times log(\hat{y}) - (1-y)\times log(1-\hat{y}) $

我们先计算输出层$ w_{11}^{(3)} $的梯度（记住$ w_{11}^{(3)} $为输出层第三层的第一个神经元的第一个参数）：

$ \begin{aligned}
\frac{\partial L(\hat{y},y)}{\partial w^{(3)}_{11}} &= \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial w^{(3)}_{11}} \\
&= (-\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}})\cdot \hat{y}(1-\hat{y})\cdot a^{(2)}_1 \\

&= (\hat{y} - y) \cdot a^{(2)}_1
\end{aligned} $

同理我们可以得到$ w_{12}^{(3)} $、$ w_{13}^{(3)} $以及$ b^{(3)}_1 $的梯度，分别为$ (\hat{y} - y) \cdot a^{(2)}_2 $、$ (\hat{y} - y) \cdot a^{(2)}_3 $和$ (\hat{y} - y) $ 。接下来我们求隐藏层的参数的梯度，我们先求$ w^{(2)}_{11} $的梯度：

$ \begin{aligned}
\frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{11}} &= \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial a^{(2)}_1} \cdot \frac{\partial a^{(2)}_1}{\partial z^{(2)}_1} \cdot \frac{\partial z^{(2)}_1}{\partial w^{(2)}_{11}} \\
&= (\hat{y}-y) \cdot w^{(3)}_{11} \cdot h'(z^{(2)}_{1}) \cdot a^{(1)}_1

\end{aligned} $

同理可得$ w^{(2)}_{12} $、$ w^{(2)}_{13} $、$ w^{(2)}_{14} $以及$ b^{(2)}_1 $的梯度分别为：

| 参数 | 梯度 |
| --- | --- |
| $ w^{(2)}_{12} $ | $ (\hat{y}-y) \cdot w^{(3)}_{11} \cdot h'(z^{(2)}_{1}) \cdot a^{(1)}_2 $ |
| $ w^{(2)}_{13} $ | $ (\hat{y}-y) \cdot w^{(3)}_{11} \cdot h'(z^{(2)}_{1}) \cdot a^{(1)}_3 $ |
| $ w^{(2)}_{14} $ | $ (\hat{y}-y) \cdot w^{(3)}_{11} \cdot h'(z^{(2)}_{1}) \cdot a^{(1)}_4 $ |
| $ b^{(2)}_1 $ | $ (\hat{y}-y) \cdot w^{(3)}_{11} \cdot h'(z^{(2)}_{1}) $ |


我们再计算$ w_{21}^{(2)} $的梯度：

$ \begin{aligned}
\frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{21}} &= \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial a^{(2)}_2} \cdot \frac{\partial a^{(2)}_2}{\partial z^{(2)}_2} \cdot \frac{\partial z^{(2)}_2}{\partial w^{(2)}_{21}} \\
&= (\hat{y}-y) \cdot w^{(3)}_{12} \cdot h'(z^{(2)}_{2}) \cdot a^{(1)}_1

\end{aligned} $ 

同理我们也可以得到$ w^{(2)}_{31} $的梯度：

$ \frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{31}}
= (\hat{y}-y) \cdot w^{(3)}_{13} \cdot h'(z^{(2)}_{3}) \cdot a^{(1)}_1 $

我们发现在计算第二层神经网络的参数的时候，我们都要计算$ \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} = \hat{y} - y $，于是我们令:

$ \delta^{(3)}_1 = \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} = \hat{y} - y $

$ \delta_i^{(2)} = \delta^{(3)}_1 w^{(3)}_{1i} h'(z^{(2)}_{i}) $

| 梯度 | $ w_{i1}^{(2)} $ | $ w_{i2}^{(2)} $ | $ w_{i3}^{(2)} $ | $ w_{i4}^{(2)} $ | $ b_{i}^{(2)} $ |
| --- | --- | --- | --- | --- | --- |
| i=1 | $ \delta_1^{(2)} a^{(1)}_1 $ | $ \delta_1^{(2)} a^{(1)}_2 $ | $ \delta_1^{(2)}  a^{(1)}_3 $ | $ \delta_1^{(2)}  a^{(1)}_4 $ | $ \delta_1^{(2)}  $ |
| i=2 | $ \delta_2^{(2)}  a^{(1)}_1 $ | $ \delta_2^{(2)}   a^{(1)}_2 $ | $ \delta_2^{(2)}  a^{(1)}_3 $ | $ \delta_2^{(2)}   a^{(1)}_4 $ | $ \delta_2^{(2)}  $ |
| i=3 | $ \delta_3^{(2)}   a^{(1)}_1 $ | $ \delta_3^{(2)}   a^{(1)}_2 $ | $ \delta_3^{(2)}   a^{(1)}_3 $ | $ \delta_3^{(2)}  a^{(1)}_4 $ | $ \delta_3^{(2)}   $ |


通用的公式可以写为：

$ \frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{ij}} =\delta_i^{(2)} a^{(1)}_j $

最后我们求第一层神经网络参数的梯度，我们先求$ w_{11}^{(1)} $的梯度。先回忆一下，$ w_{11}^{(1)} $通过和输入$ x_1 $相乘得到$ z_{1}^{(1)} $,$ z_{1}^{(1)} $通过激活函数得到$ a_1^{(1)} $。

$ \begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial w_{11}^{(1)}} &= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial w_{11}^{(1)}} \\
&= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h'(z_1^{(1)}) \cdot x_1
\end{aligned} $

然后第二层神经网络内三个神经元都使用了$ a_1^{(1)} $进行了计算，分别得到了$ z_1^{(2)} $、$ z_2^{(2)} $、$ z_3^{(2)} $，并且最后都对最终结果产生了影响。接着我们单独计算$ \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} $:

$ \begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} &= \frac{\partial L(\hat{y}, y)}{\partial z_1^{(2)}} \cdot \frac{\partial z_1^{(2)}}{\partial a_1^{(1)}} +  \frac{\partial L(\hat{y}, y)}{\partial z_2^{(2)}} \cdot \frac{\partial z_2^{(2)}}{\partial a_1^{(1)}} +  \frac{\partial L(\hat{y}, y)}{\partial z_3^{(2)}} \cdot \frac{\partial z_3^{(2)}}{\partial a_1^{(1)}} \\

&= \sum_{k=1}^3 \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial a_k^{(2)}} \cdot \frac{\partial a^{(2)}_k}{\partial z_k^{(2)}} \cdot \frac{\partial z_k^{(2)}}{\partial a_1^{(1)}} \\

&=\sum_{k=1}^3 (\hat{y}-y) \cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w_{k1}^{(2)}  \\
&= \sum_{k=1}^3 (\hat{y} - y)\cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w_{k1}^{(2)}
\end{aligned} $

将这个结果带到上面的式子，就可以得到：

$ \begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial w_{11}^{(1)}} &= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial w_{11}^{(1)}} \\
&= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h'(z_1^{(1)}) \cdot x_1 \\
&= \sum_{k=1}^3(\hat{y} - y)\cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w_{k1}^{(2)} \cdot h'(z_1^{(1)}) \cdot x_1
\end{aligned} $

接着我们计算$ w_{12}^{(1)} $以及$ b^{(1)}_1 $的梯度：

$ \begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial w_{12}^{(1)}} &= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial w_{12}^{(1)}} \\
&= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h'(z_1^{(1)}) \cdot x_2 \\
&= \sum_{k=1}^3 (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w_{k1}^{(2)} \cdot h'(z_1^{(1)}) \cdot x_2
\end{aligned} $

$ \begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial b^{(1)}_1} &= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial b^{(1)}_1} \\
&= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h'(z_1^{(1)}) \cdot 1\\
&= \sum_{k=1}^3 (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w_{k1}^{(2)} \cdot h'(z_1^{(1)})
\end{aligned} $ 

我们接着计算$ w_{21}^{(1)} $的梯度：

$ \begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial w_{21}^{(1)}} &= \frac{\partial L(\hat{y}, y)}{\partial a_{2}^{(1)}} \cdot \frac{\partial a_{2}^{(1)}}{\partial z_2^{(1)}} \cdot \frac{\partial z_2^{(1)}}{\partial w_{21}^{(1)}} \\

&= \frac{\partial L(\hat{y}, y)}{\partial a_{2}^{(1)}} \cdot h'(z_{2}^{(1)}) \cdot x_1 \\

&= \sum_{k=1}^{3}\frac{\partial L(\hat{y}, y)}{\partial z_k^{(2)}} \cdot \frac{\partial z_k^{(2)}}{\partial a_2^{(1)}} \cdot h'(z_{2}^{(1)}) \cdot x_1 \\

&= \sum_{k=1}^{3} \frac{\partial L(\hat{y}, y)}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_1^{(3)}} \cdot \frac{\partial z_1^{(3)}}{\partial a_k^{(2)}} \cdot \frac{\partial  a_k^{(2)}}{\partial z_k^{(2)}} \cdot \frac{\partial z_k^{(2)}}{\partial a_2^{(1)}} \cdot h'(z_{2}^{(1)}) \cdot x_1  \\

&= \sum_{k=1}^{3} (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w^{(2)}_{k2} \cdot h'(z_2^{(1)}) \cdot x_1
\end{aligned} $

我们令$ \delta_i^{(1)} = \sum_{k=1}^{3} (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h'(z_k^{(2)}) \cdot w^{(2)}_{ki}= \sum_{k=1}^3 \delta_i^{(2)} w_{ki}^{(2)} $，能得到以下梯度：

| 梯度 | $ w_{i1}^{(1)} $ | $ w_{i2}^{(1)} $ | $ b_{i}^{(1)} $ |
| --- | --- | --- | --- |
| i=1 | $ \delta_1^{(1)} x_1 $ | $ \delta_1^{(1)} x_2 $ | $ \delta_1^{(1)} $ |
| i=2 | $ \delta_2^{(1)} x_1 $ | $ \delta_2^{(1)} x_2 $ | $ \delta_2^{(1)} $ |
| i=3 | $ \delta_3^{(1)} x_1 $ | $ \delta_3^{(1)} x_2 $ | $ \delta_3^{(1)} $ |
| i=4 | $ \delta_4^{(1)} x_1 $ | $ \delta_4^{(1)} x_2 $ | $ \delta_4^{(1)} $ |


我们可以得到通用的神经网络权重梯度的计算公式，对于第$ l $层的第$ i $个神经元的权重$ w_{ij}^{(l)} $：

$ \frac{\partial L(\hat{y}, y)}{\partial w^{(l)}_{ij}} = \delta_i^{(l)} \cdot a_j^{(l-1)} $

其中的反向传播的误差$ \delta_i^{(l)} = (\sum_k \delta_k^{(l+1)}\cdot w_{ki}^{(l+1)})\cdot h'(z^{(l)}_i) $。

