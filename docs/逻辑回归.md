##  损失函数和代价函数
我们使用交叉熵损失（Cross Entropy Loss）函数作为逻辑回归算法的损失函数，其公式如下

$ L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) = \begin{cases}
-log(f_{\vec{w},b}(\vec{x}^{(i)})) & \text{if y=1} \\
-log(1-f_{\vec{w},b}(\vec{x}^{(i)})) & \text{if y=0} \\
\end{cases} $

其中, $ f_{\vec{w},b}(\vec{x}^{(i)}) $为 Sigmoid 函数，通过计算给出的输入和自身权重乘积（以及加上偏置），经过Sigmoid函数的处理，最后计算出一个处于(0,1）之间的概率，这个概率表示预测为真的概率。Sigmoid函数公式如下：

$ \hat{y} = f_{\vec{w}, b}(\vec{x}^{(i)}) = Sigmoid(\vec{x}^{(i)}) = \frac{1}{1+e^{-(\vec{w}^T \cdot \vec{x} ^{(i)} + b)}} $

> 其中 $ \vec{w}^T \cdot \vec{x} ^{(i)} $与 $ \sum_{j=1}^n  w_j \times x_j^{(i)} $等价，都是计算 $ w_1 \times x_1^{(i)} + ... + w_n \times x_n^{(i)} $，只不过前者为矩阵写法，表示一个一维矩阵$ \vec{w} $的转置（transpose）乘以另外一个一维矩阵$ \vec{x^{(i)}} $，而后者是常见的求和公式。使用矩阵写法不仅在书写上更方便，也更符合我们需要在NumPy中需要写的代码。
>
>   
一个小细节：为什么机器学习中总是习惯性的写成$ \vec{w}^T \cdot \vec{x}^{(i)} $，而不是$ \vec{w} \cdot \vec{x}^{(i)T} $？因为在机器学习中，我们习惯性的将$ \vec{w} $和 $ \vec{x}^{(i)} $视作形状为 $ m \times 1 $的列向量（column vector） ，于是$ \vec{w}^T \times \vec{x}^{(i)} $实际上是一个形状$ m \times 1 $，求转置后，成为一个形状为$ 1 \times m $的矩阵，然后乘以另一个形状为 $ m \times 1 $的矩阵。如果你足够了解矩阵乘法，那么你就会知道前者的计算结果是一个形状为$ 1 \times 1 $的矩阵；而后者的计算结果是一个$ m \times m $的矩阵。在这里我们只想要得到一个标量，而不需要一个矩阵。
>

交叉熵损失函数如果写成一行公式，则可以表示为（以下这个公式和上面的损失函数是完全等价的）：

$ L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) = -y\times log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1-y)\times log(1-f_{\vec{w},b}(\vec{x}^{(i)})) $

而整个数据集的代价函数，加上L2正则化项，则可以表示为：

$ J(\vec{w}, b) = \frac{1}m\sum_{i=1}^m L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) + \frac{\lambda}{2m}\sum_{j=1}^n w_j^2 $

## 计算梯度
由于逻辑回归的损失函数是一个非常复杂的复合函数，我们使用链式法则对这个函数求导，以计算得到梯度。首先我们求在代价函数在$ w_j $方向上的梯度：

$ \begin{aligned}
\frac{\partial J(\vec{w},b)}{\partial w_j} &= \frac{\partial}{\partial w_j} \frac{1}{m}\sum_{i=1}^m L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) +\frac{\partial}
{\partial w_j}  \frac{\lambda}{2m}\sum_{j=1}^n w_j^2 \\
&= \frac{1}{m}\sum_{i=1}^m \frac{\partial}{\partial w_j} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) +  \frac{\lambda}{m} w_j \\
\end{aligned} $

我们把$ \frac{\partial}{\partial w_j} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) $单独拎出来计算，并且用$ \hat{y} $代替$ f_{\vec{w}, b}(\vec{x}^{(i)}) $（仅仅是为了书写简单）。根据链式法则我们有：

 $ \begin{aligned}
\frac{\partial}{\partial w_j} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})
&= \frac{\partial}{\partial w_j} [-y^{(i)}\times log(\hat{y}^{(i)}) - (1-y^{(i)})\times log(1-\hat{y}^{(i)})] \\
&=  \frac{\partial}{\partial \hat{y}^{(i)}}[-y^{(i)}\times log(\hat{y}^{(i)}) - (1-y^{(i)})\times log(1-\hat{y}^{(i)})]\cdot \frac{\partial \hat{y}^{(i)}}{\partial w_j} 
\end{aligned} $ 

通常这里的$ log $函数的底可以是任意实数，但是为了我们计算简便，我们使用自然常数$ e $作为底：

$ \begin{aligned}
\frac{\partial}{\partial w_j} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})
&= \frac{\partial}{\partial \hat{y}^{(i)}} [-y^{(i)}\times log(\hat{y}^{(i)}) - (1-y^{(i)})\times log(1-\hat{y}^{(i)})]\cdot \frac{\partial \hat{y}^{(i)}}{\partial w_j}  \\
&= (-\frac{y^{(i)}}{\hat{y}^{(i)}} + \frac{1-y^{(i)}}{1-\hat{y}^{(i)}})\cdot \frac{\partial \hat{y}^{(i)}}{\partial w_j} 
\end{aligned} $

接着我们求$ \frac{\partial \hat{y}^{(i)}}{\partial w_j} $，我们设$ z^{(i)} = \vec{w}^T \cdot \vec{x}^{(i)} + b $，根据链式法则我们有：

$ \begin{aligned}
\frac{\partial \hat{y}^{(i)}}{\partial w_j} &= \frac{\partial\hat{y}^{(i)}}{\partial z^{(i)}} \cdot \frac{\partial z^{(i)}}{\partial w_j} \\
&= \frac{\partial}{\partial z^{(i)}} \frac{1}{1+e^{-z^{(i)}}} \cdot \frac{\partial}{\partial w_j} (\sum_{j=1}^n w_j \times x_j + b)  \\
&= \hat{y}^{(i)}(1-\hat{y}^{(i)}) x_j
\end{aligned} $

> 关于 Sigmoid 函数求导，其过程如下：
>
> $ \begin{aligned}
(\frac{1}{1+e^{-z}})' &= (\frac{e^z}{e^z + 1})' \\
&= \frac{e^z (e^z + 1) - (e^z)^2}{(e^z + 1)^2} \\
&= \frac{e^z}{e^z + 1} - (\frac{e^z}{e^z + 1})^2 \\
&= Sigmoid(z) - Sigmoid^2(z) \\
&= Sigmoid(z) (1- Sigmoid(z)) 
\end{aligned} $ 
>
> 即 $ \hat{y}(1-\hat{y}) $
>

于是我们有：

$ \begin{aligned}
\frac{\partial}{\partial w_j} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})
&= (-\frac{y^{(i)}}{\hat{y}^{(i)}} + \frac{1-y^{(i)}}{1-\hat{y}^{(i)}})\cdot \hat{y}^{(i)}(1-\hat{y}^{(i)}) x_j \\
&= [- y^{(i)}(1-\hat{y}^{(i)}) + \hat{y}^{(i)}(1-y^{(i)})] x_j \\
&= (-y^{(i)} + y^{(i)}\hat{y}^{(i)} + \hat{y}^{(i)}- y^{(i)}\hat{y}^{(i)}) x_j \\
&= (\hat{y}^{(i)} - y^{(i)}) x_j
\end{aligned} $

结果非常的amazing啊，交叉熵损失函数CEL的求导结果竟然和我们在线性回归中使用的损失函数MSE的求导结果一模一样。我们将这个求导结果带入代价函数，得到代价函数在$ w_j $方向上的梯度：

$ \frac{\partial}{\partial w_j} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})
= \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)}) x_j +  \frac{\lambda}{m} w_j $

我们接着计算这个损失函数在 $ b $方向上的偏导，求导过程几乎和上面展示的一样，唯一的不同是 $ \frac{\partial z^{(i)}}{\partial w_j} = x_j $而 $ \frac{\partial z^{(i)}}{\partial b} = 1 $(假设 $ z^{(i)} = \vec{w}^T \cdot \vec{x}^{(i)} + b $)：

$ \begin{aligned}
\frac{\partial J(\vec{w},b)}{\partial b} 

&= \frac{\partial}{\partial b} \frac{1}{m}\sum_{i=1}^m L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) +\frac{\partial}
{\partial b}  \frac{\lambda}{2m}\sum_{j=1}^n w_j^2 \\

&= \frac{1}{m}\sum_{i=1}^m \frac{\partial}{\partial b} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) \\

&= \frac{1}{m}\sum_{i=1}^m ( \frac{\partial}{\partial b} [-y^{(i)} \times log(\hat{y}^{(i)}) - (1-y^{(i)})\times log(1-\hat{y}^{(i)})] ) \\

&= \frac{1}{m}\sum_{i=1}^m ( \frac{\partial}{\partial \hat{y}^{(i)}} [-y^{(i)}\times log(\hat{y}^{(i)}) - (1-y^{(i)})\times log(1-\hat{y}^{(i)})]\cdot \frac{\partial \hat{y}^{(i)}}{\partial b} ) \\

&= \frac{1}{m}\sum_{i=1}^m \{\frac{\partial}{\partial \hat{y}^{(i)}} [-y^{(i)}\times log(\hat{y}^{(i)}) - (1-y^{(i)})\times log(1-\hat{y}^{(i)})] 
\cdot  \frac{\partial}{\partial z^{(i)}} \frac{1}{1+e^{-z^{(i)}}} \cdot \frac{\partial z^{(i)}}{\partial b} \} \\

&=  \frac{1}{m}\sum_{i=1}^m \{(-\frac{y^{(i)}}{\hat{y}^{(i)}} + \frac{1-y^{(i)}}{1-\hat{y}^{(i)}})\cdot\hat{y}^{(i)}(1-\hat{y}^{(i)}) \cdot 1\} \\

&=  \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})
\end{aligned} $



于是我们更新梯度的公式为：

$ \begin{aligned}
w_j' &= w_j- \alpha \frac{\partial J(\vec{w},b)}{\partial w_j} \\
&= w_j - \alpha(\frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)}) x_j +  \frac{\lambda}{m} w_j) \\
&= w_j- \frac{\alpha}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)}) x_j - \frac{\alpha \lambda}{m} w_j
\\
\\
b' &= b - \alpha \frac{\partial J(\vec{w},b)}{\partial b} \\
&= b - \frac{\alpha }{m}\sum_{i=1}^m(\hat{y}^{(i)} - y^{(i)})

\end{aligned} $

## 多项式逻辑回归
逻辑回归只能处理二分类问题，如果想要预测有多个标签的结果，单个逻辑回归模型就无能为力了。处理多标签分类问题时，最直观、最容易想到的办法是先将其中几个聚集为一类，分类，然后再在每个子类内继续分类。例如我们现在需要分类的结果有 [0, 1, 2, 3] 四类，那么我们可以先将0和1分为一类，3和4分为另一类，这样问题就简化成了多个二分类问题：

![](https://cdn.nlark.com/yuque/0/2025/png/45785720/1752349137934-5b2912db-74f7-47d0-9531-7f2f0acae18e.png)

> 在处理多分类的问题的时候，我们首先需要将预测标签编码成 one-hot 编码。例如在上例中，我们想要预测的结果有四类[0, 1, 2, 3]，但是我们实际的标签——假设这条数据的类别是2——则是[0, 0, 1, 0]，分别对应这[0, 1, 2, 3]。或者假设我们要预测的标签为 [红色， 绿色， 蓝色]，而这单条样本的标签为绿色，则我们需要将$ y $编码成 [0，1，0] （注意，我们所有的标签都需要遵循一样的次序）。
>

这里我们需要连续训练三个逻辑回归模型，如果需要分的类别越多，则需要训练的模型也越多。有更简单的方法吗？有的！在面对多分类问题时，我们可以使用多项式逻辑回归（Polynomial Logistic Regression）。多项式逻辑回归的核心是 Softmax 函数，我们用 Softmax 函数取代而二项式逻辑回归中的 Sigmoid 函数：

 $ Softmax(\vec{x}, b) = \frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}} $

这里的$ z_k $表示每个类别的输入和其权重的乘积再加上偏置，即$ z_k = \vec{w_k}^T \vec{x} + b_k $。假设我们有三个类别需要分类，分别是 [0, 1, 2] 三个类别：

1. 我们首先初始化三组权重，分别是 $ \vec{w}_0 $和$ b_0 $、$ \vec{w}_1 $和$ b_1 $、$ \vec{w}_2 $和$ b_2 $；
2. 接着我们计算权重和输入的乘积 $ z_0 = \vec{w}_0^T\vec{x} + b_0 $、$ z_1 = \vec{w}_1^T\vec{x} + b_1 $、$ z_2 = \vec{w}_2^T\vec{x} + b_2 $;
3. 最后我们通过 Softmax函数，计算出每个分类的概率$ p_0 = \frac{e^{z_0}}{e^{z_0} + e^{z_1} + e^{z_2}} $、$ p_1 = \frac{e^{z_1}}{e^{z_0} + e^{z_1} + e^{z_2}} $、$ p_2 = \frac{e^{z_2}}{e^{z_0} + e^{z_1} + e^{z_2}} $。这样我们就能得到三个概率，分别对应着预测为0、1、2三类的概率，并且$ p_0 + p_1 + p_2 = 1 $。

> 我们使用softmax函数主要是为了将预测的结果归一化为三个加起来等于1的概率，那为什么使用以 $ e $为底的 Softmax 函数，而不是直接使用$ p_0 = \frac{z_0}{z_0 + z_1+ z_2} $呢？后者不也可以将多个输出归一到和为1的多个概率吗？使用以 $ e $ 为底的指数函数的目的主要有两个：一个是可以保证结果中不会出现负数，二个是可以利用指数爆炸的性质，放大多个预测结果之间的差异，使其更贴合我们期望的“其中一个类别的预测结果趋近于1，而其他几个类别的预测结果接近于0”的预测结果。至于为什么底是$ e $?当然是为了我们后面计算梯度的时候求导方便啦！
>

这个时候我们得到了三个概率，那我们该怎么计算梯度，更新我们模型的权重参数呢？在多分类问题中，我们沿用逻辑回归中的损失函数 $ L(\hat{y}, y) = \sum_{k}^K-y_klog\hat{y_k}
 $。假设数据的真实标签是0，那么我们期望我们的模型的输出为 [1, 0, 0]（分别对应$ p_0 $$ p_1 $$ p_2 $），但是我们模型实际输出为[0.34, 0.33, 0.33]，那么我们的模型这一轮训练的损失为$ (- 1 \times log 0.34) + (- 0 \times log 0.33) + (- 0 \times log 0.33) $。后面两项为0，这意味着在多项式分类中，我们只关心标签为真的项的损失。

## 计算多项式逻辑回归的梯度
跟之前求导梯度不一样的是，我们现在一共有$ K $组权重，并且每组权重内有$ m $个权重以及一个偏置$ b $。（假设我们预测的结果一共有$ K $类，并且每条数据的维度为$ m $维）。

$ \left.
\begin{array}{c}
    \overbrace{w_{1}^{1}, w_{2}^{1} , \cdots , w_{m}^{1}}^{m\text{ 个 }w} , b^1 \\
    \\
   w_{1}^{2} , w_{2}^{2} , \cdots , w_{m}^{2} , b^2 \\
    \\
    \vdots \\
    \\
    w_{1}^{K} , w_{2}^{K} , \cdots , w_{m}^{K} , b^K
\end{array}
\right\} K\text{组} $

跟之前不同的是，之前的权重只有一个维度，而这里的权重成为了一个二维的矩阵，我们接下来要分别求出这个矩阵中每一个参数的梯度。在计算Softmax函数梯度之前，我们要根据标签$ y $的值分成两种情况，分别分成$ i = k $ 的时候和 $ i \ne k $的时候。因为当$ i=k $的时候，Softmax函数的分子内包含我们要求的参数的偏导；而$ i\ne k $ 的时候，Softmax函数的分子内不包含我们要求的参数的梯度，因此我们可以将分子上的式子视作一个常数。

由于我们需要根据标签的值分成多种情况求导，当预测标签的值不一样的时候，我们计算各个参数的梯度的方式也不一样。所以和之前不一样的是，我们需要先计算单个样本上的梯度，再将所有梯度总和起来，最后将总和起来的梯度更新到权重上去——而不是将所有样本总和在一起批量求得梯度。

我们先求当$ i=k $的时候，$ w_j^i $和$ b^i $的梯度（这里$ i $表示$ 0..k $的第几组权重，而$ j $表示$ 1..m $的每组权重中的第几个权重）：

$ \begin{aligned}
\frac{\partial L}{ \partial w_j^i} 

&= \frac{\partial - \sum_k^Ky_klog\hat{y_k}}{\partial \hat{y_k}}\frac{\partial \hat{y_k}}{\partial z^i} \frac{\partial z^i}{\partial w_j^i} \\

&= -\sum_k^K y_k \cdot \frac{1}{\hat{y_k}} \cdot \hat{y_i}(1-\hat{y_i}) \cdot x_j \\

&= -(1- \hat{y_i}) x_j \\

&= (\hat{y_i} - 1) x_j
 
\end{aligned} $

同理可得：

$ \frac{\partial L}{ \partial b^i} = (\hat{y_i} - 1) $

我们再求当 $ i \ne k $的时候，$ w_j^i $和$ b^i $的梯度：

$ \begin{aligned}
\frac{\partial L}{ \partial w_j^i} 

&= \frac{\partial - \sum_k^Ky_klog\hat{y_k}}{\partial \hat{y_k}}\frac{\partial \hat{y_k}}{\partial z^i} \frac{\partial z^i}{\partial w_j^i} \\

&= -\sum_k^K y_k \cdot \frac{1}{\hat{y_k}} \cdot (-\hat{y_k}\hat{y_i}) \cdot x_j \\

&=  \hat{y_i} x_j
 
\end{aligned} $ 

同理可得：

$ \frac{\partial L}{ \partial b^i} = \hat{y_i} $

>  $ \sum_k^K y_k = 1 $ ，$ y_k $代表了数据实际的标签，是一个[0,0,...,1,...,0,0] 的数组，所以所有 $ y_k $加起来等于一。  
关于Softmax函数求导，其求导过程如下，当 $ i=k $的时候：
>
> $ \begin{aligned} 
\frac{\partial \hat{y_k}}{\partial z^i} &= (\frac{e^{z_i}}{\sum e^{z_k}})' \\
&= \frac{e^{z_i}\sum e^{z_k} - (e^{z_i})^2}{(\sum e^{z_k})^2} \\
&= \hat{y_i} (1-\hat{y_i})
\end{aligned} $
>
> 当 $ i \ne k $的时候：
>
> $ \begin{aligned} 
\frac{\partial \hat{y_k}}{\partial z^k} &= (\frac{e^{z_k}}{\sum e^{z_k}})' \\
&= \frac{0 \cdot\sum e^{z_k} - e^{z_i}e^{z_k}}{(\sum e^{z_k})^2} \\
&= -\hat{y_i}\hat{y_k}
\end{aligned} $
>
> 其中 $ \frac{\partial}{\partial z_i} \sum e^{z_k} = \frac{\partial}{\partial z_i} (e^{z_1} + ...+ e^{z_i}+...+ e^{z_K}) = e^{z_i} $
>

$ \hat{y_i} x_j $可以写成 $ (\hat{y_i} - 0 )x_j $的形式，当 $ i = k $的时候，$ \hat{y} = 1 $；而当$ i \ne k $的时候，$ \hat{y} = 0 $。于是梯度计算又回到了我们熟悉的形式：

$ \frac{\partial L}{ \partial w_j^i} = (\hat{y_i} - y_i) x_j $

$ \frac{\partial L}{ \partial b^i} = \hat{y_i} - y_i $



