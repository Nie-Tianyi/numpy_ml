## 损失函数和代价函数
在线性回归中，我们通常使用MSE（Mean Square Error, 均方误差） 作为我们算法的损失函数，其基本思想为衡量预测值和真实结果之间的绝对距离。MSE的公式为：$ MSE = \frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 $。同时我们为了防止函数过拟合，我们引入正则表达式。在这里，我们先用L2正则表达式作为例子，L2正则表达式为：$ \frac{\lambda}{2m}\sum^n_{j=1}{w^2_j} $。结合这两项，我们函数的代价函数可以表达为：

$ J(\vec{w},b) = \frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum^n_{j=1}{w^2_j} $。

在反向传播中，我们需要计算梯度，以更新算法的权重：

$ w_j' = w_j- \alpha \frac{\partial J(\vec{w},b)}{\partial w_j} $

$ b' = b - \alpha \frac{\partial J(\vec{w},b)}{\partial b} $

## 计算梯度
$ \frac{\partial J(\vec{w},b)}{\partial w_j}  = \frac{\partial}{\partial w_j}( \frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum^n_{j=1}{w^2_j})  $

根据求导规则，对** x+y求导** 等于 **对x求导** 加上 **对y求导**，即 (x + y)' = x' + y' :

$  = \frac{\partial}{\partial w_j}\frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2   + \frac{\partial}{\partial w_j}\frac{\lambda}{2m}\sum^n_{j=1}{w^2_j} $

### MSE梯度
我们分别计算两个部分，我们先计算第一部分：

$ 
\frac{\partial}{\partial w_j}\frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 = \frac{1}{2m} \sum_{i=1}^{m} \frac{\partial}{\partial w_j}(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})^2 $

根据链式法则，我们有 f[g(x)]' = f(g(x)) * g'(x)，于是

$ \begin{aligned}
 &= \frac{1}{2m} \sum_{i=1}^{m} [2 \times (f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) \times \frac{\partial f_{\vec{w},b}(\vec{x}^{(i)})}{\partial w_j}] \\
&= \frac{1}{m} \sum_{i=1}^{m} [(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) \times \frac{\partial f_{\vec{w},b}(\vec{x}^{(i)})}{\partial w_j}]
\end{aligned} $

其中：

$ \begin{aligned}
\frac{\partial f_{\vec{w},b}(\vec{x}^{(i)})}{\partial w_j} 
&= \frac{\partial}{\partial w_i^{(i)}}  (w_1 x_1^{(i)} + ...w_jx_j^{(i)} + ... + w_nx_n^{(i)} + b) \\
&= x_j^{(i)}
\end{aligned} $

所以，第一部分求导结果如下

$ = \frac{1}{m} \sum_{i=1}^{m} [(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) x_j^{(i)}] $

### 正则表达式梯度
我们再对正则表达式部分求导：

$ \begin{aligned}
\frac{\partial}{\partial w_j}\frac{\lambda}{2m}\sum^n_{j=1}{w^2_j} &= \frac{\lambda}{2m} \frac{\partial}{\partial w_j} (w_1^2 + .. w_j^2 + ... + w_n^2)\\
&=  \frac{\lambda}{2m} \times 2 \times w_j \\
&= \frac{\lambda}{m}w_j
\end{aligned} $

正则表达式和MSE部分梯度总结起来就是：

$ \frac{\partial J(\vec{w},b)}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} [(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) x_j^{(i)}] + \frac{\lambda}{m}w_j $

### 计算Bias梯度
接着我们对b求导

$ \begin{aligned}
\frac{\partial J(\vec{w},b)}{\partial b} &=  \frac{\partial}{\partial b}( \frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum^n_{j=1}{w^2_j}) \\ 
&=\frac{\partial}{\partial b}\frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2   + \frac{\partial}{\partial b}\frac{\lambda}{2m}\sum^n_{j=1}{w^2_j} \\
&= \frac{1}{2m}\times 2 \times \sum^m_{i=1}  (f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})  \frac{\partial}{\partial b}f_{\vec{w},b}(\vec{x}^{(i)}) + 0 \\
&= \frac{1}{m} \sum^m_{i=1} (f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) \times 1 \\
&= \frac{1}{m} \sum^m_{i=1} (f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})

\end{aligned} $

其中：

$ \begin{aligned}
\frac{\partial}{\partial b}f_{\vec{w},b}(\vec{x}^{(i)}) &= \frac{\partial}{\partial b}  w_1 x_1^{(i)} + ...w_jx_j^{(i)} + ... + w_nx_n^{(i)} + b \\
&= 1
\end{aligned} $

所以更新权重的算法现在为：

$ w_j' = w_j- \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) x_j^{(i)} - \alpha \frac{\lambda}{m}w_j $

$ b' = b - \alpha \frac{1}{m} (f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) $

## 深入理解L2正则表达式
通过观察上面$ w_j $的更新公式，我们可以观察到：

$ \begin{aligned}
w_j' &= \textcolor{red}{w_j}- \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) x_j^{(i)} \textcolor{red}{ - \alpha \frac{\lambda}{m}w_j }\\
& = \textcolor{red}{(1-\frac{\alpha \lambda}{m}) w_j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}) x_j^{(i)}
\end{aligned} $

其中$ \alpha $为学习率，$ \lambda $为正则参数，m 为训练样本容量。可以看到L2正则表达式实际上在做的是，每一轮训练都把现有的$ w_j $固定乘以一个稍微小于1的系数——越大的权重将减少的越多——因此我们能避免个别权重过大，让模型过于复杂。

